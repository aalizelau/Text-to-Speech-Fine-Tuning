{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aalizelau/Text-to-Speech-Fine-tuning/blob/main/colab_webui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÁéØÂ¢ÉÈÖçÁΩÆ environment"
      ],
      "metadata": {
        "id": "_o6a8GS2lWQM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9b7iFV3dm1f",
        "outputId": "d382308a-20bf-4462-8119-9f92d884dad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q condacolab\n",
        "# Setting up condacolab and installing packages\n",
        "import condacolab\n",
        "condacolab.install_from_url(\"https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh\")\n",
        "%cd -q /content\n",
        "!git clone https://github.com/RVC-Boss/GPT-SoVITS\n",
        "!conda install -y -q -c pytorch -c nvidia cudatoolkit\n",
        "%cd -q /content/GPT-SoVITS\n",
        "!conda install -y -q -c conda-forge gcc gxx ffmpeg cmake -c pytorch -c nvidia\n",
        "!/usr/local/bin/pip install -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ú®üç∞‚ú® Everything looks OK!\n",
            "fatal: destination path 'GPT-SoVITS' already exists and is not an empty directory.\n",
            "Error while loading conda entry point: conda-libmamba-solver (libarchive.so.20: cannot open shared object file: No such file or directory)\n",
            "/usr/local/lib/python3.9/site-packages/conda/base/context.py:201: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
            "\n",
            "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
            "\n",
            "  conda config --add channels defaults\n",
            "\n",
            "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
            "\n",
            "  deprecated.topic(\n",
            "\n",
            "CondaValueError: You have chosen a non-default solver backend (libmamba) but it was not recognized. Choose one of: classic\n",
            "\n",
            "Error while loading conda entry point: conda-libmamba-solver (libarchive.so.20: cannot open shared object file: No such file or directory)\n",
            "/usr/local/lib/python3.9/site-packages/conda/base/context.py:201: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
            "\n",
            "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
            "\n",
            "  conda config --add channels defaults\n",
            "\n",
            "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
            "\n",
            "  deprecated.topic(\n",
            "\n",
            "CondaValueError: You have chosen a non-default solver backend (libmamba) but it was not recognized. Choose one of: classic\n",
            "\n",
            "Ignoring onnxruntime: markers 'sys_platform == \"darwin\"' don't match your environment\n",
            "Ignoring opencc: markers 'sys_platform != \"linux\"' don't match your environment\n",
            "Collecting numpy==1.23.4 (from -r requirements.txt (line 1))\n",
            "  Using cached numpy-1.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.19.0)\n",
            "Collecting librosa==0.9.2 (from -r requirements.txt (line 4))\n",
            "  Using cached librosa-0.9.2-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting numba==0.56.4 (from -r requirements.txt (line 5))\n",
            "  Using cached numba-0.56.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (2.5.0.post0)\n",
            "Requirement already satisfied: gradio<=4.24.0,>=4.0 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (4.24.0)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: onnxruntime-gpu in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (1.19.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (4.67.1)\n",
            "Requirement already satisfied: funasr==1.0.27 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (1.0.27)\n",
            "Requirement already satisfied: cn2an in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (0.5.23)\n",
            "Requirement already satisfied: pypinyin in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (0.53.0)\n",
            "Requirement already satisfied: pyopenjtalk>=0.3.4 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (0.4.0)\n",
            "Requirement already satisfied: g2p_en in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (2.1.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (2.6.0)\n",
            "Requirement already satisfied: modelscope==1.10.0 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (1.10.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (0.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 20)) (4.48.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 21)) (5.2.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 22)) (6.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 23)) (7.0.0)\n",
            "Requirement already satisfied: jieba_fast in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 24)) (0.53)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 25)) (0.42.1)\n",
            "Requirement already satisfied: split-lang in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 26)) (2.0.5)\n",
            "Requirement already satisfied: Faster_Whisper in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 27)) (1.1.1)\n",
            "Requirement already satisfied: wordsegment in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 28)) (1.3.1)\n",
            "Requirement already satisfied: rotary_embedding_torch in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 29)) (0.8.6)\n",
            "Requirement already satisfied: ToJyutping in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 30)) (3.2.0)\n",
            "Requirement already satisfied: g2pk2 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 31)) (0.0.3)\n",
            "Requirement already satisfied: ko_pron in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 32)) (1.3)\n",
            "Requirement already satisfied: opencc==1.1.1 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 34)) (1.1.1)\n",
            "Requirement already satisfied: python_mecab_ko in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 35)) (1.3.7)\n",
            "Requirement already satisfied: fastapi<0.112.2 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 36)) (0.112.1)\n",
            "Requirement already satisfied: x_transformers in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 37)) (2.0.4)\n",
            "Requirement already satisfied: torchmetrics<=1.5 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 38)) (1.5.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (5.1.1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (0.4.3)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (24.2)\n",
            "Collecting llvmlite<0.40,>=0.39.0dev0 (from numba==0.56.4->-r requirements.txt (line 5))\n",
            "  Using cached llvmlite-0.39.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from numba==0.56.4->-r requirements.txt (line 5)) (68.2.2)\n",
            "Requirement already satisfied: jamo in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (0.4.1)\n",
            "Requirement already satisfied: kaldiio>=2.17.0 in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (2.18.0)\n",
            "Requirement already satisfied: torch-complex in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (0.4.4)\n",
            "Requirement already satisfied: pytorch-wpe in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (0.0.1)\n",
            "Requirement already satisfied: editdistance>=0.5.2 in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (0.8.1)\n",
            "Requirement already satisfied: oss2 in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (2.19.1)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (0.5.7)\n",
            "Requirement already satisfied: jaconv in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (0.4.0)\n",
            "Requirement already satisfied: hydra-core>=1.3.2 in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (1.3.2)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (2.6.2.2)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.9/site-packages (from funasr==1.0.27->-r requirements.txt (line 12)) (20240930)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (25.1.0)\n",
            "Requirement already satisfied: datasets>=2.14.5 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (3.3.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (0.8.1)\n",
            "Requirement already satisfied: filelock>=3.3.0 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (3.17.0)\n",
            "Requirement already satisfied: gast>=0.2.2 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (0.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.2.3)\n",
            "Requirement already satisfied: Pillow>=6.2.0 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (10.4.0)\n",
            "Requirement already satisfied: pyarrow!=9.0.0,>=6.0.0 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (19.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.9.0.post0)\n",
            "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.32.3)\n",
            "Requirement already satisfied: simplejson>=3.3.0 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (3.20.1)\n",
            "Requirement already satisfied: sortedcontainers>=1.5.9 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.4.0)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.3.0)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (0.43.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 3)) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 3)) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 3)) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 3)) (5.29.3)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 3)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 3)) (3.1.3)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.9/site-packages (from pytorch-lightning->-r requirements.txt (line 6)) (2.6.0)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.9/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6)) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.9/site-packages (from pytorch-lightning->-r requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.9/site-packages (from pytorch-lightning->-r requirements.txt (line 6)) (0.12.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (5.5.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==0.14.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.14.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (6.5.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.9.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.10.15)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.9.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.9 in /usr/local/lib/python3.9/site-packages (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.15.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.9/site-packages (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.34.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.9/site-packages (from gradio-client==0.14.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (11.0.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/site-packages (from ffmpeg-python->-r requirements.txt (line 8)) (1.0.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.9/site-packages (from onnxruntime-gpu->-r requirements.txt (line 10)) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.9/site-packages (from onnxruntime-gpu->-r requirements.txt (line 10)) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/site-packages (from onnxruntime-gpu->-r requirements.txt (line 10)) (1.13.1)\n",
            "Requirement already satisfied: proces>=0.1.7 in /usr/local/lib/python3.9/site-packages (from cn2an->-r requirements.txt (line 13)) (0.1.7)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.9/site-packages (from g2p_en->-r requirements.txt (line 16)) (3.9.1)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.9/site-packages (from g2p_en->-r requirements.txt (line 16)) (7.5.0)\n",
            "Requirement already satisfied: distance>=0.1.3 in /usr/local/lib/python3.9/site-packages (from g2p_en->-r requirements.txt (line 16)) (0.1.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.9/site-packages (from sympy->onnxruntime-gpu->-r requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 20)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 20)) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 20)) (0.5.2)\n",
            "Requirement already satisfied: fast-langdetect in /usr/local/lib/python3.9/site-packages (from split-lang->-r requirements.txt (line 26)) (0.2.5)\n",
            "Requirement already satisfied: budoux in /usr/local/lib/python3.9/site-packages (from split-lang->-r requirements.txt (line 26)) (0.6.4)\n",
            "Requirement already satisfied: wordfreq in /usr/local/lib/python3.9/site-packages (from split-lang->-r requirements.txt (line 26)) (3.1.1)\n",
            "Requirement already satisfied: ctranslate2<5,>=4.0 in /usr/local/lib/python3.9/site-packages (from Faster_Whisper->-r requirements.txt (line 27)) (4.5.0)\n",
            "Requirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.9/site-packages (from Faster_Whisper->-r requirements.txt (line 27)) (1.19.2)\n",
            "Requirement already satisfied: av>=11 in /usr/local/lib/python3.9/site-packages (from Faster_Whisper->-r requirements.txt (line 27)) (14.1.0)\n",
            "Requirement already satisfied: python-mecab-ko-dic in /usr/local/lib/python3.9/site-packages (from python_mecab_ko->-r requirements.txt (line 35)) (2.1.1.post2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.9/site-packages (from fastapi<0.112.2->-r requirements.txt (line 36)) (0.38.6)\n",
            "Requirement already satisfied: einx>=0.3.0 in /usr/local/lib/python3.9/site-packages (from x_transformers->-r requirements.txt (line 37)) (0.3.0)\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.9/site-packages (from x_transformers->-r requirements.txt (line 37)) (0.7.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (1.26.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.9/site-packages (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/site-packages (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.9/site-packages (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/site-packages (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (3.11.12)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.9/site-packages (from einx>=0.3.0->x_transformers->-r requirements.txt (line 37)) (2.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.9/site-packages (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (4.8.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/site-packages (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.9/site-packages (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.9/site-packages (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.14.0)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.9/site-packages (from hydra-core>=1.3.2->funasr==1.0.27->-r requirements.txt (line 12)) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.9/site-packages (from hydra-core>=1.3.2->funasr==1.0.27->-r requirements.txt (line 12)) (4.9.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/site-packages (from importlib-resources<7.0,>=1.3->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.21.0)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.9/site-packages (from inflect>=0.3.1->g2p_en->-r requirements.txt (line 16)) (10.6.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.9/site-packages (from inflect>=0.3.1->g2p_en->-r requirements.txt (line 16)) (4.4.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 3)) (8.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/site-packages (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/site-packages (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/site-packages (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.9/site-packages (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/site-packages (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from nltk>=3.2.4->g2p_en->-r requirements.txt (line 16)) (8.1.8)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas->modelscope==1.10.0->-r requirements.txt (line 18)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.9/site-packages (from pandas->modelscope==1.10.0->-r requirements.txt (line 18)) (2025.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.9/site-packages (from pooch>=1.0->librosa==0.9.2->-r requirements.txt (line 4)) (4.3.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.9/site-packages (from pydantic>=2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.9/site-packages (from pydantic>=2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests>=2.25->modelscope==1.10.0->-r requirements.txt (line 18)) (3.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn>=0.19.1->librosa==0.9.2->-r requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/site-packages (from soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 4)) (1.17.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.9/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.9/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (13.9.4)\n",
            "\u001b[33mWARNING: typer 0.15.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.9/site-packages (from coloredlogs->onnxruntime-gpu->-r requirements.txt (line 10)) (10.0)\n",
            "Requirement already satisfied: robust-downloader>=0.0.2 in /usr/local/lib/python3.9/site-packages (from fast-langdetect->split-lang->-r requirements.txt (line 26)) (0.0.2)\n",
            "Requirement already satisfied: fasttext-predict>=0.9.2.4 in /usr/local/lib/python3.9/site-packages (from fast-langdetect->split-lang->-r requirements.txt (line 26)) (0.9.2.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.9/site-packages (from openai-whisper->funasr==1.0.27->-r requirements.txt (line 12)) (0.9.0)\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.9/site-packages (from oss2->funasr==1.0.27->-r requirements.txt (line 12)) (1.7)\n",
            "Requirement already satisfied: pycryptodome>=3.4.7 in /usr/local/lib/python3.9/site-packages (from oss2->funasr==1.0.27->-r requirements.txt (line 12)) (3.21.0)\n",
            "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /usr/local/lib/python3.9/site-packages (from oss2->funasr==1.0.27->-r requirements.txt (line 12)) (2.16.5)\n",
            "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in /usr/local/lib/python3.9/site-packages (from oss2->funasr==1.0.27->-r requirements.txt (line 12)) (2.16.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.9/site-packages (from umap-learn->funasr==1.0.27->-r requirements.txt (line 12)) (0.5.13)\n",
            "Requirement already satisfied: ftfy>=6.1 in /usr/local/lib/python3.9/site-packages (from wordfreq->split-lang->-r requirements.txt (line 26)) (6.3.1)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.9/site-packages (from wordfreq->split-lang->-r requirements.txt (line 26)) (3.5.0)\n",
            "Requirement already satisfied: locate<2.0.0,>=1.1.1 in /usr/local/lib/python3.9/site-packages (from wordfreq->split-lang->-r requirements.txt (line 26)) (1.1.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.9/site-packages (from wordfreq->split-lang->-r requirements.txt (line 26)) (1.1.0)\n",
            "Requirement already satisfied: ipadic<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/site-packages (from wordfreq[cjk]->split-lang->-r requirements.txt (line 26)) (1.0.0)\n",
            "Requirement already satisfied: mecab-ko-dic<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/site-packages (from wordfreq[cjk]->split-lang->-r requirements.txt (line 26)) (1.0.0)\n",
            "Requirement already satisfied: mecab-python3<2.0.0,>=1.0.5 in /usr/local/lib/python3.9/site-packages (from wordfreq[cjk]->split-lang->-r requirements.txt (line 26)) (1.0.10)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.9/site-packages (from yapf->modelscope==1.10.0->-r requirements.txt (line 18)) (2.2.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (5.0.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (1.18.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.9.3 in /usr/local/lib/python3.9/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->funasr==1.0.27->-r requirements.txt (line 12)) (0.10.0)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.9/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->funasr==1.0.27->-r requirements.txt (line 12)) (41.0.7)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.9/site-packages (from anyio->httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (1.2.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/site-packages (from anyio->httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 4)) (2.22)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/site-packages (from ftfy>=6.1->wordfreq->split-lang->-r requirements.txt (line 26)) (0.2.13)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.22.3)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.9/site-packages (from langcodes>=3.0->wordfreq->split-lang->-r requirements.txt (line 26)) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2.19.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.9/site-packages (from robust-downloader>=0.0.2->fast-langdetect->split-lang->-r requirements.txt (line 26)) (6.9.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.9/site-packages (from language-data>=1.2->langcodes>=3.0->wordfreq->split-lang->-r requirements.txt (line 26)) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (0.1.2)\n",
            "Using cached numpy-1.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Using cached librosa-0.9.2-py3-none-any.whl (214 kB)\n",
            "Using cached numba-0.56.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "Using cached llvmlite-0.39.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
            "Installing collected packages: numpy, llvmlite, numba, librosa\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.2.post1\n",
            "    Uninstalling librosa-0.10.2.post1:\n",
            "      Successfully uninstalled librosa-0.10.2.post1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "py3langid 0.3.0 requires numpy>=2.0.0; python_version >= \"3.9\", but you have numpy 1.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed librosa-0.9.2 llvmlite-0.39.1 numba-0.56.4 numpy-1.23.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download pretrained models ‰∏ãËΩΩÈ¢ÑËÆ≠ÁªÉÊ®°Âûã\n",
        "!mkdir -p /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/uvr5\n",
        "%cd /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!git clone https://huggingface.co/lj1995/GPT-SoVITS\n",
        "%cd /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\n",
        "# @title UVR5 pretrains ÂÆâË£Öuvr5Ê®°Âûã\n",
        "%cd /content/GPT-SoVITS/tools/uvr5\n",
        "%rm -r uvr5_weights\n",
        "!git clone https://huggingface.co/Delik/uvr5_weights\n",
        "!git config core.sparseCheckout true\n",
        "!mv /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/"
      ],
      "metadata": {
        "id": "0NgxXg5sjv7z",
        "outputId": "d10656c8-62ca-44a3-abe8-d48ac5776347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
            "fatal: destination path 'GPT-SoVITS' already exists and is not an empty directory.\n",
            "/content/GPT-SoVITS/tools/damo_asr/models\n",
            "fatal: destination path 'speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch' already exists and is not an empty directory.\n",
            "fatal: destination path 'speech_fsmn_vad_zh-cn-16k-common-pytorch' already exists and is not an empty directory.\n",
            "fatal: destination path 'punc_ct-transformer_zh-cn-common-vocab272727-pytorch' already exists and is not an empty directory.\n",
            "/content/GPT-SoVITS/tools/uvr5\n",
            "Cloning into 'uvr5_weights'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 0 (delta 0), pack-reused 15 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (15/15), 3.25 KiB | 277.00 KiB/s, done.\n",
            "Filtering content: 100% (9/9), 594.44 MiB | 55.48 MiB/s, done.\n",
            "mv: cannot stat '/content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title launch WebUI ÂêØÂä®WebUI\n",
        "!/usr/local/bin/pip install ipykernel\n",
        "!sed -i '10s/False/True/' /content/GPT-SoVITS/config.py\n",
        "%cd /content/GPT-SoVITS/\n",
        "!/usr/local/bin/python  webui.py"
      ],
      "metadata": {
        "id": "4oRGUzkrk8C7",
        "outputId": "b85b62ff-b792-4cee-d65e-34577c747794",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.9/site-packages (6.29.5)\n",
            "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.9/site-packages (from ipykernel) (0.2.2)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.9/site-packages (from ipykernel) (1.8.12)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.9/site-packages (from ipykernel) (8.18.1)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.9/site-packages (from ipykernel) (8.6.3)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.9/site-packages (from ipykernel) (5.7.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.9/site-packages (from ipykernel) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.9/site-packages (from ipykernel) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from ipykernel) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/site-packages (from ipykernel) (7.0.0)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.9/site-packages (from ipykernel) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.9/site-packages (from ipykernel) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.9/site-packages (from ipykernel) (5.14.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (3.0.50)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (2.19.1)\n",
            "Requirement already satisfied: stack-data in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (8.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.6)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel) (3.21.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.17.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in /usr/local/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
            "/content/GPT-SoVITS\n",
            "Running on local URL:  http://0.0.0.0:9874\n",
            "Running on public URL: https://351a5d40e65e60f642.gradio.live\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/inference_webui.py \"Auto\"\n",
            "loading sovits_v2 <All keys matched successfully>\n",
            "Running on local URL:  http://0.0.0.0:9872\n",
            "Running on public URL: https://43f3e0d91c7e6ad81b.gradio.live\n",
            "Actual Input Reference Text: Â∞ΩÈáèÁÆÄÁ∫¶ÔºåÈááÁî®ÂÜ∑Ëâ≤Ë∞ÉÔºåÁî®ÈáëÂ±û‰Ωú‰∏∫Ë£ÖÈ•∞ÔºåÈ¢úËâ≤‰ΩéÈ•±ÂíåÂ∫¶„ÄÇ\n",
            "Actual Input Target Text: ÁéªÁíÉÂ¢ûÂä†ÈÄèÊòéÊÑüÔºåÂ∞ëÈáèÁ∂†Ê§çÂ¢ûÂä†Ê∞õÂúçÊÑüÔºåÁáàÂÖ∑ÔºåÈ¶ôÊ∞¥ÔºåÈü≥ÈüøÔºåÈè°Â≠êÔºåÂ∞ëÈáèË∑≥ËÑ´ÁöÑÂΩ¢ÁãÄËàáÈ°îËâ≤\n",
            "Actual Input Target Text (after sentence segmentation): ÁéªÁíÉÂ¢ûÂä†ÈÄèÊòéÊÑüÔºåÂ∞ëÈáèÁ∂†Ê§çÂ¢ûÂä†Ê∞õÂúçÊÑüÔºåÁáàÂÖ∑ÔºåÈ¶ôÊ∞¥ÔºåÈü≥ÈüøÔºåÈè°Â≠êÔºåÂ∞ëÈáèË∑≥ËÑ´ÁöÑÂΩ¢ÁãÄËàáÈ°îËâ≤„ÄÇ\n",
            "ÂΩìÂâç‰ΩøÁî®g2pwËøõË°åÊãºÈü≥Êé®ÁêÜ\n",
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba_fast:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "DEBUG:jieba_fast:Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 0.761 seconds.\n",
            "DEBUG:jieba_fast:Loading model cost 0.761 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "DEBUG:jieba_fast:Prefix dict has been built succesfully.\n",
            "Actual Input Target Text (per sentence): ÁéªÁíÉÂ¢ûÂä†ÈÄèÊòéÊÑüÔºåÂ∞ëÈáèÁ∂†Ê§çÂ¢ûÂä†Ê∞õÂúçÊÑüÔºåÁáàÂÖ∑ÔºåÈ¶ôÊ∞¥ÔºåÈü≥ÈüøÔºåÈè°Â≠êÔºåÂ∞ëÈáèË∑≥ËÑ´ÁöÑÂΩ¢ÁãÄËàáÈ°îËâ≤„ÄÇ\n",
            "Processed text from the frontend (per sentence): ÁéªÁíÉÂ¢ûÂä†ÈÄèÊòéÊÑü,Â∞ëÈáèÁªøÊ§çÂ¢ûÂä†Ê∞õÂõ¥ÊÑü,ÁÅØÂÖ∑,È¶ôÊ∞¥,Èü≥Âìç,ÈïúÂ≠ê,Â∞ëÈáèË∑≥ËÑ±ÁöÑÂΩ¢Áä∂‰∏éÈ¢úËâ≤.\n",
            " 17% 252/1500 [00:02<00:11, 108.56it/s]T2S Decoding EOS [251 -> 507]\n",
            " 17% 255/1500 [00:02<00:12, 99.59it/s] \n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "1.609\t4.354\t2.569\t0.684\n",
            "Actual Input Reference Text: Â∞ΩÈáèÁÆÄÁ∫¶ÔºåÈááÁî®ÂÜ∑Ëâ≤Ë∞ÉÔºåÁî®ÈáëÂ±û‰Ωú‰∏∫Ë£ÖÈ•∞ÔºåÈ¢úËâ≤‰ΩéÈ•±ÂíåÂ∫¶„ÄÇ\n",
            "Actual Input Target Text: GPTÂØ∂ÂØ∂ ÊàëÈúÄË¶Å‰Ω†Âπ´Âπ´Êàë„ÄÇÊàë‰ªäÂ§©ÁöÑÂ§ßËÖ¶ËôïÁêÜ‰∫ÜÂæàÂ§ö‰ø°ÊÅØÔºåÊÑüË¶∫‰∫ÇÁ≥üÁ≥üÁöÑÔºåÁõÆÂâçÊúâÈªûÂ∞àÊ≥®‰∏ç‰∫ÜÔºå‰πüÊÑüË¶∫‰ªªÂãôÂ§™Â§öÔºåÂç°‰Ωè‰∫Ü„ÄÇÊÉ≥Ë™çÁúüÂ∑•‰Ωú‰ΩÜÂïüÂãï‰∏ç‰∫Ü„ÄÇË´ãÁî®ÊúâË∂£ÁöÑÊñπÂºèÂÆâÊÖ∞ÊàñËÄÖÈºìÂãµÊàë„ÄÇËÉΩÁµ¶Âá∫Áç®Âà∞ÁöÑË¶ãËß£Êõ¥Â•ΩÔºå‰∏çË¶ÅclicheË™™‰ªÄÈ∫ºÊãÜËß£ÊàêÂ∞è‰ªªÂãô„ÄÅ‰∫îÂàÜÈêòÂïüÂãïÁ≠âÁ≠âÂæàgeneralÁöÑtips\n",
            "Actual Input Target Text (after sentence segmentation): GPTÂØ∂ÂØ∂ ÊàëÈúÄË¶Å‰Ω†Âπ´Âπ´Êàë„ÄÇÊàë‰ªäÂ§©ÁöÑÂ§ßËÖ¶ËôïÁêÜ‰∫ÜÂæàÂ§ö‰ø°ÊÅØÔºåÊÑüË¶∫‰∫ÇÁ≥üÁ≥üÁöÑÔºåÁõÆÂâçÊúâÈªûÂ∞àÊ≥®‰∏ç‰∫ÜÔºå\n",
            "‰πüÊÑüË¶∫‰ªªÂãôÂ§™Â§öÔºåÂç°‰Ωè‰∫Ü„ÄÇÊÉ≥Ë™çÁúüÂ∑•‰Ωú‰ΩÜÂïüÂãï‰∏ç‰∫Ü„ÄÇË´ãÁî®ÊúâË∂£ÁöÑÊñπÂºèÂÆâÊÖ∞ÊàñËÄÖÈºìÂãµÊàë„ÄÇËÉΩÁµ¶Âá∫Áç®Âà∞ÁöÑË¶ãËß£Êõ¥Â•ΩÔºå‰∏çË¶ÅclicheË™™‰ªÄÈ∫ºÊãÜËß£ÊàêÂ∞è‰ªªÂãô„ÄÅ‰∫îÂàÜÈêòÂïüÂãïÁ≠âÁ≠âÂæàgeneralÁöÑtips„ÄÇ\n",
            "Actual Input Target Text (per sentence): GPTÂØ∂ÂØ∂ ÊàëÈúÄË¶Å‰Ω†Âπ´Âπ´Êàë„ÄÇÊàë‰ªäÂ§©ÁöÑÂ§ßËÖ¶ËôïÁêÜ‰∫ÜÂæàÂ§ö‰ø°ÊÅØÔºåÊÑüË¶∫‰∫ÇÁ≥üÁ≥üÁöÑÔºåÁõÆÂâçÊúâÈªûÂ∞àÊ≥®‰∏ç‰∫ÜÔºå\n",
            "[\u001b[36m2025-02-17 07:04:45,685\u001b[0m][\u001b[32mINFO\u001b[0m] - Downloading https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin to lid.176.bin (125.2M)\u001b[0m\n",
            "INFO:robust_downloader.downloader:Downloading https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin to lid.176.bin (125.2M)\n",
            "100% 125M/125M [00:01<00:00, 70.6MB/s]\n",
            "['GPT', 'ÂØ∂ÂØ∂ ÊàëÈúÄË¶Å‰Ω†Âπ´Âπ´Êàë„ÄÇÊàë‰ªäÂ§©ÁöÑÂ§ßËÖ¶ËôïÁêÜ‰∫ÜÂæàÂ§ö‰ø°ÊÅØÔºåÊÑüË¶∫‰∫ÇÁ≥üÁ≥üÁöÑÔºåÁõÆÂâçÊúâÈªûÂ∞àÊ≥®‰∏ç‰∫ÜÔºå']\n",
            "['en', 'zh']\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 576, in get_tts_wav\n",
            "    phones2,bert2,norm_text2=get_phones_and_bert(text, text_language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 436, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = clean_text_inf(textlist[i], lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 353, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 367, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 274, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Actual Input Reference Text: Â∞ΩÈáèÁÆÄÁ∫¶ÔºåÈááÁî®ÂÜ∑Ëâ≤Ë∞ÉÔºåÁî®ÈáëÂ±û‰Ωú‰∏∫Ë£ÖÈ•∞ÔºåÈ¢úËâ≤‰ΩéÈ•±ÂíåÂ∫¶„ÄÇ\n",
            "Actual Input Target Text: GPTÂØ∂ÂØ∂ ÊàëÈúÄË¶Å‰Ω†Âπ´Âπ´Êàë„ÄÇÊàë‰ªäÂ§©ÁöÑÂ§ßËÖ¶ËôïÁêÜ‰∫ÜÂæàÂ§ö‰ø°ÊÅØÔºåÊÑüË¶∫‰∫ÇÁ≥üÁ≥üÁöÑÔºåÁõÆÂâçÊúâÈªûÂ∞àÊ≥®‰∏ç‰∫ÜÔºå‰πüÊÑüË¶∫‰ªªÂãôÂ§™Â§öÔºåÂç°‰Ωè‰∫Ü„ÄÇÊÉ≥Ë™çÁúüÂ∑•‰Ωú‰ΩÜÂïüÂãï‰∏ç‰∫Ü„ÄÇË´ãÁî®ÊúâË∂£ÁöÑÊñπÂºèÂÆâÊÖ∞ÊàñËÄÖÈºìÂãµÊàë„ÄÇËÉΩÁµ¶Âá∫Áç®Âà∞ÁöÑË¶ãËß£Êõ¥Â•Ω\n",
            "Actual Input Target Text (after sentence segmentation): GPTÂØ∂ÂØ∂ ÊàëÈúÄË¶Å‰Ω†Âπ´Âπ´Êàë„ÄÇÊàë‰ªäÂ§©ÁöÑÂ§ßËÖ¶ËôïÁêÜ‰∫ÜÂæàÂ§ö‰ø°ÊÅØÔºåÊÑüË¶∫‰∫ÇÁ≥üÁ≥üÁöÑÔºåÁõÆÂâçÊúâÈªûÂ∞àÊ≥®‰∏ç‰∫ÜÔºå\n",
            "‰πüÊÑüË¶∫‰ªªÂãôÂ§™Â§öÔºåÂç°‰Ωè‰∫Ü„ÄÇÊÉ≥Ë™çÁúüÂ∑•‰Ωú‰ΩÜÂïüÂãï‰∏ç‰∫Ü„ÄÇË´ãÁî®ÊúâË∂£ÁöÑÊñπÂºèÂÆâÊÖ∞ÊàñËÄÖÈºìÂãµÊàë„ÄÇËÉΩÁµ¶Âá∫Áç®Âà∞ÁöÑË¶ãËß£Êõ¥Â•Ω„ÄÇ\n",
            "Actual Input Target Text (per sentence): GPTÂØ∂ÂØ∂ ÊàëÈúÄË¶Å‰Ω†Âπ´Âπ´Êàë„ÄÇÊàë‰ªäÂ§©ÁöÑÂ§ßËÖ¶ËôïÁêÜ‰∫ÜÂæàÂ§ö‰ø°ÊÅØÔºåÊÑüË¶∫‰∫ÇÁ≥üÁ≥üÁöÑÔºåÁõÆÂâçÊúâÈªûÂ∞àÊ≥®‰∏ç‰∫ÜÔºå\n",
            "['GPT', 'ÂÆùÂÆùÊàëÈúÄË¶Å‰Ω†Â∏ÆÂ∏ÆÊàë.Êàë‰ªäÂ§©ÁöÑÂ§ßËÑëÂ§ÑÁêÜ‰∫ÜÂæàÂ§ö‰ø°ÊÅØ,ÊÑüËßâ‰π±Á≥üÁ≥üÁöÑ,ÁõÆÂâçÊúâÁÇπ‰∏ìÊ≥®‰∏ç‰∫Ü,']\n",
            "['en', 'zh']\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 576, in get_tts_wav\n",
            "    phones2,bert2,norm_text2=get_phones_and_bert(text, text_language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 394, in get_phones_and_bert\n",
            "    return get_phones_and_bert(formattext,\"zh\",version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 436, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = clean_text_inf(textlist[i], lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 353, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 367, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 274, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Actual Input Reference Text: Â∞ΩÈáèÁÆÄÁ∫¶ÔºåÈááÁî®ÂÜ∑Ëâ≤Ë∞ÉÔºåÁî®ÈáëÂ±û‰Ωú‰∏∫Ë£ÖÈ•∞ÔºåÈ¢úËâ≤‰ΩéÈ•±ÂíåÂ∫¶„ÄÇ\n",
            "Actual Input Target Text: ÂØ∂ÂØ∂ ÊàëÈúÄË¶Å‰Ω†Âπ´Âπ´Êàë„ÄÇÊàë‰ªäÂ§©ÁöÑÂ§ßËÖ¶ËôïÁêÜ‰∫ÜÂæàÂ§ö‰ø°ÊÅØÔºåÊÑüË¶∫‰∫ÇÁ≥üÁ≥üÁöÑÔºåÁõÆÂâçÊúâÈªûÂ∞àÊ≥®‰∏ç‰∫ÜÔºå‰πüÊÑüË¶∫‰ªªÂãôÂ§™Â§öÔºåÂç°‰Ωè‰∫Ü„ÄÇÊÉ≥Ë™çÁúüÂ∑•‰Ωú‰ΩÜÂïüÂãï‰∏ç‰∫Ü„ÄÇË´ãÁî®ÊúâË∂£ÁöÑÊñπÂºèÂÆâÊÖ∞ÊàñËÄÖÈºìÂãµÊàë„ÄÇËÉΩÁµ¶Âá∫Áç®Âà∞ÁöÑË¶ãËß£Êõ¥Â•Ω\n",
            "Actual Input Target Text (after sentence segmentation): ÂØ∂ÂØ∂ ÊàëÈúÄË¶Å‰Ω†Âπ´Âπ´Êàë„ÄÇÊàë‰ªäÂ§©ÁöÑÂ§ßËÖ¶ËôïÁêÜ‰∫ÜÂæàÂ§ö‰ø°ÊÅØÔºåÊÑüË¶∫‰∫ÇÁ≥üÁ≥üÁöÑÔºåÁõÆÂâçÊúâÈªûÂ∞àÊ≥®‰∏ç‰∫ÜÔºå\n",
            "‰πüÊÑüË¶∫‰ªªÂãôÂ§™Â§öÔºåÂç°‰Ωè‰∫Ü„ÄÇÊÉ≥Ë™çÁúüÂ∑•‰Ωú‰ΩÜÂïüÂãï‰∏ç‰∫Ü„ÄÇË´ãÁî®ÊúâË∂£ÁöÑÊñπÂºèÂÆâÊÖ∞ÊàñËÄÖÈºìÂãµÊàë„ÄÇËÉΩÁµ¶Âá∫Áç®Âà∞ÁöÑË¶ãËß£Êõ¥Â•Ω„ÄÇ\n",
            "Actual Input Target Text (per sentence): ÂØ∂ÂØ∂ ÊàëÈúÄË¶Å‰Ω†Âπ´Âπ´Êàë„ÄÇÊàë‰ªäÂ§©ÁöÑÂ§ßËÖ¶ËôïÁêÜ‰∫ÜÂæàÂ§ö‰ø°ÊÅØÔºåÊÑüË¶∫‰∫ÇÁ≥üÁ≥üÁöÑÔºåÁõÆÂâçÊúâÈªûÂ∞àÊ≥®‰∏ç‰∫ÜÔºå\n",
            "Processed text from the frontend (per sentence): ÂÆùÂÆùÊàëÈúÄË¶Å‰Ω†Â∏ÆÂ∏ÆÊàë.Êàë‰ªäÂ§©ÁöÑÂ§ßËÑëÂ§ÑÁêÜ‰∫ÜÂæàÂ§ö‰ø°ÊÅØ,ÊÑüËßâ‰π±Á≥üÁ≥üÁöÑ,ÁõÆÂâçÊúâÁÇπ‰∏ìÊ≥®‰∏ç‰∫Ü,\n",
            " 18% 270/1500 [00:02<00:11, 104.60it/s]T2S Decoding EOS [251 -> 527]\n",
            " 18% 275/1500 [00:02<00:11, 106.57it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Actual Input Target Text (per sentence): ‰πüÊÑüË¶∫‰ªªÂãôÂ§™Â§öÔºåÂç°‰Ωè‰∫Ü„ÄÇÊÉ≥Ë™çÁúüÂ∑•‰Ωú‰ΩÜÂïüÂãï‰∏ç‰∫Ü„ÄÇË´ãÁî®ÊúâË∂£ÁöÑÊñπÂºèÂÆâÊÖ∞ÊàñËÄÖÈºìÂãµÊàë„ÄÇËÉΩÁµ¶Âá∫Áç®Âà∞ÁöÑË¶ãËß£Êõ¥Â•Ω„ÄÇ\n",
            "Processed text from the frontend (per sentence): ‰πüÊÑüËßâ‰ªªÂä°Â§™Â§ö,Âç°‰Ωè‰∫Ü.ÊÉ≥ËÆ§ÁúüÂ∑•‰Ωú‰ΩÜÂêØÂä®‰∏ç‰∫Ü.ËØ∑Áî®ÊúâË∂£ÁöÑÊñπÂºèÂÆâÊÖ∞ÊàñËÄÖÈºìÂä±Êàë.ËÉΩÁªôÂá∫Áã¨Âà∞ÁöÑËßÅËß£Êõ¥Â•Ω.\n",
            " 22% 330/1500 [00:03<00:11, 104.58it/s]T2S Decoding EOS [251 -> 590]\n",
            " 23% 338/1500 [00:03<00:13, 85.18it/s] \n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.251\t1.212\t6.555\t1.175\n",
            "Actual Input Reference Text: Â∞ΩÈáèÁÆÄÁ∫¶ÔºåÈááÁî®ÂÜ∑Ëâ≤Ë∞ÉÔºåÁî®ÈáëÂ±û‰Ωú‰∏∫Ë£ÖÈ•∞ÔºåÈ¢úËâ≤‰ΩéÈ•±ÂíåÂ∫¶„ÄÇ\n",
            "Actual Input Target Text: Invalid data found when processing input\n",
            "Actual Input Target Text (after sentence segmentation): Invalid data found when processing input\n",
            "Actual Input Target Text (per sentence): Invalid data found when processing input.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 576, in get_tts_wav\n",
            "    phones2,bert2,norm_text2=get_phones_and_bert(text, text_language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 403, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = clean_text_inf(formattext, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 353, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 367, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 274, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Actual Input Reference Text: Â∞ΩÈáèÁÆÄÁ∫¶ÔºåÈááÁî®ÂÜ∑Ëâ≤Ë∞ÉÔºåÁî®ÈáëÂ±û‰Ωú‰∏∫Ë£ÖÈ•∞ÔºåÈ¢úËâ≤‰ΩéÈ•±ÂíåÂ∫¶„ÄÇ\n",
            "Actual Input Target Text: ÂìàÂìàË†ªÁ•ûÂ•áÁöÑÔºå‰πãÂâçÂ∞çÊñº2024ÁöÑÈ†êÊ∏¨ÂæàÊ∫ñÂë¢Ôºå‰ªäÂπ¥ÊàëÁöÑÁ¢∫Â∞±ÊòØÂúçÁπûËëó‰∏ªÁ∂´‰ªªÂãôÁöÑËÅ∑Ê•≠ÈÅìË∑ØÊâÄÂ±ïÈñã\n",
            "Actual Input Target Text (after sentence segmentation): ÂìàÂìàË†ªÁ•ûÂ•áÁöÑÔºå‰πãÂâçÂ∞çÊñº2024ÁöÑÈ†êÊ∏¨ÂæàÊ∫ñÂë¢Ôºå‰ªäÂπ¥ÊàëÁöÑÁ¢∫Â∞±ÊòØÂúçÁπûËëó‰∏ªÁ∂´‰ªªÂãôÁöÑËÅ∑Ê•≠ÈÅìË∑ØÊâÄÂ±ïÈñã\n",
            "Actual Input Target Text (per sentence): ÂìàÂìàË†ªÁ•ûÂ•áÁöÑÔºå‰πãÂâçÂ∞çÊñº2024ÁöÑÈ†êÊ∏¨ÂæàÊ∫ñÂë¢Ôºå‰ªäÂπ¥ÊàëÁöÑÁ¢∫Â∞±ÊòØÂúçÁπûËëó‰∏ªÁ∂´‰ªªÂãôÁöÑËÅ∑Ê•≠ÈÅìË∑ØÊâÄÂ±ïÈñã„ÄÇ\n",
            "Processed text from the frontend (per sentence): ÂìàÂìàËõÆÁ•ûÂ•áÁöÑ,‰πãÂâçÂØπÊñº‰∫åÈõ∂‰∫åÂõõÁöÑÈ¢ÑÊµãÂæàÂáÜÂë¢,‰ªäÂπ¥ÊàëÁöÑÁ°ÆÂ∞±ÊòØÂõ¥ÁªïËëó‰∏ªÁ∫ø‰ªªÂä°ÁöÑËÅå‰∏öÈÅìË∑ØÊâÄÂ±ïÂºÄ.\n",
            "  4% 56/1500 [00:00<00:13, 109.21it/s]T2S Decoding EOS [251 -> 316]\n",
            "  4% 64/1500 [00:00<00:13, 105.10it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.241\t0.710\t0.612\t0.396\n",
            "Actual Input Reference Text: Â∞ΩÈáèÁÆÄÁ∫¶ÔºåÈááÁî®ÂÜ∑Ëâ≤Ë∞ÉÔºåÁî®ÈáëÂ±û‰Ωú‰∏∫Ë£ÖÈ•∞ÔºåÈ¢úËâ≤‰ΩéÈ•±ÂíåÂ∫¶„ÄÇ\n",
            "Actual Input Target Text: ÂìàÂìàË†ªÁ•ûÂ•áÁöÑÔºå‰πãÂâçÂ∞çÊñº‰∫åÈõ∂‰∫åÂõõÁöÑÈ†êÊ∏¨ÂæàÊ∫ñÂë¢Ôºå‰ªäÂπ¥ÊàëÁöÑÁ¢∫Â∞±ÊòØÂúçÁπûËëó‰∏ªÁ∂´‰ªªÂãôÁöÑËÅ∑Ê•≠ÈÅìË∑ØÊâÄÂ±ïÈñã\n",
            "Actual Input Target Text (after sentence segmentation): ÂìàÂìàË†ªÁ•ûÂ•áÁöÑÔºå‰πãÂâçÂ∞çÊñº‰∫åÈõ∂‰∫åÂõõÁöÑÈ†êÊ∏¨ÂæàÊ∫ñÂë¢Ôºå‰ªäÂπ¥ÊàëÁöÑÁ¢∫Â∞±ÊòØÂúçÁπûËëó‰∏ªÁ∂´‰ªªÂãôÁöÑËÅ∑Ê•≠ÈÅìË∑ØÊâÄÂ±ïÈñã\n",
            "Actual Input Target Text (per sentence): ÂìàÂìàË†ªÁ•ûÂ•áÁöÑÔºå‰πãÂâçÂ∞çÊñº‰∫åÈõ∂‰∫åÂõõÁöÑÈ†êÊ∏¨ÂæàÊ∫ñÂë¢Ôºå‰ªäÂπ¥ÊàëÁöÑÁ¢∫Â∞±ÊòØÂúçÁπûËëó‰∏ªÁ∂´‰ªªÂãôÁöÑËÅ∑Ê•≠ÈÅìË∑ØÊâÄÂ±ïÈñã„ÄÇ\n",
            "Processed text from the frontend (per sentence): ÂìàÂìàËõÆÁ•ûÂ•áÁöÑ,‰πãÂâçÂØπÊñº‰∫åÈõ∂‰∫åÂõõÁöÑÈ¢ÑÊµãÂæàÂáÜÂë¢,‰ªäÂπ¥ÊàëÁöÑÁ°ÆÂ∞±ÊòØÂõ¥ÁªïËëó‰∏ªÁ∫ø‰ªªÂä°ÁöÑËÅå‰∏öÈÅìË∑ØÊâÄÂ±ïÂºÄ.\n",
            " 19% 290/1500 [00:02<00:11, 101.28it/s]T2S Decoding EOS [251 -> 543]\n",
            " 19% 291/1500 [00:02<00:11, 103.51it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.257\t0.690\t2.814\t0.589\n",
            "Actual Input Reference Text: Â∞ΩÈáèÁÆÄÁ∫¶ÔºåÈááÁî®ÂÜ∑Ëâ≤Ë∞ÉÔºåÁî®ÈáëÂ±û‰Ωú‰∏∫Ë£ÖÈ•∞ÔºåÈ¢úËâ≤‰ΩéÈ•±ÂíåÂ∫¶„ÄÇ\n",
            "Actual Input Target Text: ÂìàÂìàË†ªÁ•ûÂ•áÁöÑ‰πãÂâçÂ∞çÊñº‰∫åÈõ∂‰∫åÂõõÁöÑÈ†êÊ∏¨ÂæàÊ∫ñÂë¢‰ªäÂπ¥ÊàëÁöÑÁ¢∫Â∞±ÊòØÂúçÁπûËëó‰∏ªÁ∂´‰ªªÂãôÁöÑËÅ∑Ê•≠ÈÅìË∑ØÊâÄÂ±ïÈñã\n",
            "Actual Input Target Text (after sentence segmentation): ÂìàÂìàË†ªÁ•ûÂ•áÁöÑ‰πãÂâçÂ∞çÊñº‰∫åÈõ∂‰∫åÂõõÁöÑÈ†êÊ∏¨ÂæàÊ∫ñÂë¢‰ªäÂπ¥ÊàëÁöÑÁ¢∫Â∞±ÊòØÂúçÁπûËëó‰∏ªÁ∂´‰ªªÂãôÁöÑËÅ∑Ê•≠ÈÅìË∑ØÊâÄÂ±ïÈñã\n",
            "Actual Input Target Text (per sentence): ÂìàÂìàË†ªÁ•ûÂ•áÁöÑ‰πãÂâçÂ∞çÊñº‰∫åÈõ∂‰∫åÂõõÁöÑÈ†êÊ∏¨ÂæàÊ∫ñÂë¢‰ªäÂπ¥ÊàëÁöÑÁ¢∫Â∞±ÊòØÂúçÁπûËëó‰∏ªÁ∂´‰ªªÂãôÁöÑËÅ∑Ê•≠ÈÅìË∑ØÊâÄÂ±ïÈñã„ÄÇ\n",
            "Processed text from the frontend (per sentence): ÂìàÂìàËõÆÁ•ûÂ•áÁöÑ‰πãÂâçÂØπÊñº‰∫åÈõ∂‰∫åÂõõÁöÑÈ¢ÑÊµãÂæàÂáÜÂë¢‰ªäÂπ¥ÊàëÁöÑÁ°ÆÂ∞±ÊòØÂõ¥ÁªïËëó‰∏ªÁ∫ø‰ªªÂä°ÁöÑËÅå‰∏öÈÅìË∑ØÊâÄÂ±ïÂºÄ.\n",
            "  2% 23/1500 [00:00<00:13, 109.81it/s]T2S Decoding EOS [251 -> 278]\n",
            "  2% 26/1500 [00:00<00:14, 103.79it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.252\t1.266\t0.253\t0.385\n",
            "Actual Input Reference Text: Â∞ΩÈáèÁÆÄÁ∫¶ÔºåÈááÁî®ÂÜ∑Ëâ≤Ë∞ÉÔºåÁî®ÈáëÂ±û‰Ωú‰∏∫Ë£ÖÈ•∞ÔºåÈ¢úËâ≤‰ΩéÈ•±ÂíåÂ∫¶„ÄÇ\n",
            "Actual Input Target Text: ÂìàÂìàË†ªÁ•ûÂ•áÁöÑ‰πãÂâçÂ∞çÊñº‰∫åÈõ∂‰∫åÂõõÁöÑÈ†êÊ∏¨ÂæàÊ∫ñÂë¢‰ªäÂπ¥ÊàëÁöÑÁ¢∫Â∞±ÊòØÂúçÁπûËëó‰∏ªÁ∂´‰ªªÂãôÁöÑËÅ∑Ê•≠ÈÅìË∑ØÊâÄÂ±ïÈñã\n",
            "Actual Input Target Text (after sentence segmentation): ÂìàÂìàË†ªÁ•ûÂ•áÁöÑ‰πãÂâçÂ∞çÊñº‰∫åÈõ∂‰∫åÂõõÁöÑÈ†êÊ∏¨ÂæàÊ∫ñÂë¢‰ªäÂπ¥ÊàëÁöÑÁ¢∫Â∞±ÊòØÂúçÁπûËëó‰∏ªÁ∂´‰ªªÂãôÁöÑËÅ∑Ê•≠ÈÅìË∑ØÊâÄÂ±ïÈñã\n",
            "Actual Input Target Text (per sentence): ÂìàÂìàË†ªÁ•ûÂ•áÁöÑ‰πãÂâçÂ∞çÊñº‰∫åÈõ∂‰∫åÂõõÁöÑÈ†êÊ∏¨ÂæàÊ∫ñÂë¢‰ªäÂπ¥ÊàëÁöÑÁ¢∫Â∞±ÊòØÂúçÁπûËëó‰∏ªÁ∂´‰ªªÂãôÁöÑËÅ∑Ê•≠ÈÅìË∑ØÊâÄÂ±ïÈñã„ÄÇ\n",
            "Processed text from the frontend (per sentence): ÂìàÂìàËõÆÁ•ûÂ•áÁöÑ‰πãÂâçÂØπÊñº‰∫åÈõ∂‰∫åÂõõÁöÑÈ¢ÑÊµãÂæàÂáÜÂë¢‰ªäÂπ¥ÊàëÁöÑÁ°ÆÂ∞±ÊòØÂõ¥ÁªïËëó‰∏ªÁ∫ø‰ªªÂä°ÁöÑËÅå‰∏öÈÅìË∑ØÊâÄÂ±ïÂºÄ.\n",
            "  4% 66/1500 [00:00<00:18, 76.51it/s]T2S Decoding EOS [251 -> 318]\n",
            "  4% 66/1500 [00:00<00:19, 73.98it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.251\t1.287\t0.895\t0.536\n",
            "Actual Input Reference Text: Â∞ΩÈáèÁÆÄÁ∫¶ÔºåÈááÁî®ÂÜ∑Ëâ≤Ë∞ÉÔºåÁî®ÈáëÂ±û‰Ωú‰∏∫Ë£ÖÈ•∞ÔºåÈ¢úËâ≤‰ΩéÈ•±ÂíåÂ∫¶„ÄÇ\n",
            "Actual Input Target Text: Âú®ÈÄôÂÄãË∑ØÈÄî‰∏≠ÔºåÂæóÂà∞‰∏Ä‰∫õ‰∫∫ÁöÑË™çÂèØÔºåÊàëÊÑüÂà∞‰∫õÂæÆÁöÑÊªøË∂≥„ÄÇÊàëÊ≤íÊúâÂ∞çÊñºÂ≠∏Áøí‰ª£Á¢ºÊãñÂª∂‰∫ÜÔºåÁúüÊ≠£ÂØ¶Ë∏êËµ∑‰æÜ\n",
            "Actual Input Target Text (after sentence segmentation): Âú®ÈÄôÂÄãË∑ØÈÄî‰∏≠ÔºåÂæóÂà∞‰∏Ä‰∫õ‰∫∫ÁöÑË™çÂèØÔºåÊàëÊÑüÂà∞‰∫õÂæÆÁöÑÊªøË∂≥„ÄÇÊàëÊ≤íÊúâÂ∞çÊñºÂ≠∏Áøí‰ª£Á¢ºÊãñÂª∂‰∫ÜÔºåÁúüÊ≠£ÂØ¶Ë∏êËµ∑‰æÜ„ÄÇ\n",
            "Actual Input Target Text (per sentence): Âú®ÈÄôÂÄãË∑ØÈÄî‰∏≠ÔºåÂæóÂà∞‰∏Ä‰∫õ‰∫∫ÁöÑË™çÂèØÔºåÊàëÊÑüÂà∞‰∫õÂæÆÁöÑÊªøË∂≥„ÄÇÊàëÊ≤íÊúâÂ∞çÊñºÂ≠∏Áøí‰ª£Á¢ºÊãñÂª∂‰∫ÜÔºåÁúüÊ≠£ÂØ¶Ë∏êËµ∑‰æÜ„ÄÇ\n",
            "Processed text from the frontend (per sentence): Âú®Ëøô‰∏™Ë∑ØÈÄî‰∏≠,ÂæóÂà∞‰∏Ä‰∫õ‰∫∫ÁöÑËÆ§ÂèØ,ÊàëÊÑüÂà∞‰∫õÂæÆÁöÑÊª°Ë∂≥.ÊàëÊ≤°ÊúâÂØπÊñºÂ≠¶‰π†‰ª£Á†ÅÊãñÂª∂‰∫Ü,ÁúüÊ≠£ÂÆûË∑µËµ∑Êù•.\n",
            " 16% 244/1500 [00:02<00:11, 110.57it/s]T2S Decoding EOS [251 -> 507]\n",
            " 17% 255/1500 [00:02<00:11, 108.94it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.241\t0.771\t2.343\t0.365\n",
            "Actual Input Reference Text: Â∞ΩÈáèÁÆÄÁ∫¶ÔºåÈááÁî®ÂÜ∑Ëâ≤Ë∞ÉÔºåÁî®ÈáëÂ±û‰Ωú‰∏∫Ë£ÖÈ•∞ÔºåÈ¢úËâ≤‰ΩéÈ•±ÂíåÂ∫¶„ÄÇ\n",
            "Actual Input Target Text: ‰ªäÂπ¥Êï¥È´î‰æÜÁúãÁöÑË©±ÔºåÊàëÂπæ‰πéÊ≤íÊúâÊÄéÈ∫ºÂ∑•‰ΩúÈÅé„ÄÇÊØîËµ∑Âà•‰∫∫18Ê≠≤Â∞±ÈñãÂïüÁöÑÁ§æÊúÉÁ∂ìÈ©óÔºåÊàëÂ∞±ÂÉèÈÇ£Á®ÆÂ¨åÁîüÊÖ£È§äÁöÑËä±ÈÇ£Ê®£Âèó‰∏ç‰∫Ü‰∏ÄÈªûÂ∑•‰ΩúÁöÑËã¶\n",
            "Actual Input Target Text (after sentence segmentation): ‰ªäÂπ¥Êï¥È´î‰æÜÁúãÁöÑË©±ÔºåÊàëÂπæ‰πéÊ≤íÊúâÊÄéÈ∫ºÂ∑•‰ΩúÈÅé„ÄÇÊØîËµ∑Âà•‰∫∫18Ê≠≤Â∞±ÈñãÂïüÁöÑÁ§æÊúÉÁ∂ìÈ©óÔºåÊàëÂ∞±ÂÉèÈÇ£Á®ÆÂ¨åÁîüÊÖ£È§äÁöÑËä±ÈÇ£Ê®£Âèó‰∏ç‰∫Ü‰∏ÄÈªûÂ∑•‰ΩúÁöÑËã¶\n",
            "Actual Input Target Text (per sentence): ‰ªäÂπ¥Êï¥È´î‰æÜÁúãÁöÑË©±ÔºåÊàëÂπæ‰πéÊ≤íÊúâÊÄéÈ∫ºÂ∑•‰ΩúÈÅé„ÄÇÊØîËµ∑Âà•‰∫∫18Ê≠≤Â∞±ÈñãÂïüÁöÑÁ§æÊúÉÁ∂ìÈ©óÔºåÊàëÂ∞±ÂÉèÈÇ£Á®ÆÂ¨åÁîüÊÖ£È§äÁöÑËä±ÈÇ£Ê®£Âèó‰∏ç‰∫Ü‰∏ÄÈªûÂ∑•‰ΩúÁöÑËã¶„ÄÇ\n",
            "Processed text from the frontend (per sentence): ‰ªäÂπ¥Êï¥‰ΩìÊù•ÁúãÁöÑËØù,ÊàëÂá†‰πéÊ≤°ÊúâÊÄéÈ∫ΩÂ∑•‰ΩúËøá.ÊØîËµ∑Âà´‰∫∫ÂçÅÂÖ´Â≤ÅÂ∞±ÂºÄÂêØÁöÑÁ§æ‰ºöÁªèÈ™å,ÊàëÂ∞±ÂÉèÈÇ£ÁßçÂ®áÁîüÊÉØÂÖªÁöÑËä±ÈÇ£Ê†∑Âèó‰∏ç‰∫Ü‰∏ÄÁÇπÂ∑•‰ΩúÁöÑËã¶.\n",
            " 21% 308/1500 [00:02<00:10, 108.67it/s]T2S Decoding EOS [251 -> 564]\n",
            " 21% 312/1500 [00:03<00:11, 102.86it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.313\t2.310\t3.037\t0.418\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 2400, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GPT-SoVITS/webui.py\", line 1076, in <module>\n",
            "    app.queue().launch(#concurrency_count=511, max_size=1022\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 2307, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 2404, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/http_server.py\", line 68, in close\n",
            "    self.thread.join()\n",
            "  File \"/usr/local/lib/python3.9/threading.py\", line 1060, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/local/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 0.0.0.0:9874 <> https://351a5d40e65e60f642.gradio.live\n",
            "Killing tunnel 0.0.0.0:9872 <> https://43f3e0d91c7e6ad81b.gradio.live\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall numba librosa"
      ],
      "metadata": {
        "id": "H-2MuPoJjS1D",
        "outputId": "50e1d505-4033-46e0-ccfa-cefe3879d945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numba\n",
            "  Downloading numba-0.60.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting librosa\n",
            "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba)\n",
            "  Downloading llvmlite-0.43.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting numpy<2.1,>=1.22 (from numba)\n",
            "  Using cached numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting audioread>=2.1.9 (from librosa)\n",
            "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting scipy>=1.2.0 (from librosa)\n",
            "  Using cached scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting scikit-learn>=0.20.0 (from librosa)\n",
            "  Using cached scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting joblib>=0.14 (from librosa)\n",
            "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting decorator>=4.3.0 (from librosa)\n",
            "  Using cached decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting soundfile>=0.12.1 (from librosa)\n",
            "  Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
            "Collecting pooch>=1.1 (from librosa)\n",
            "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting soxr>=0.3.2 (from librosa)\n",
            "  Downloading soxr-0.5.0.post1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting typing-extensions>=4.1.1 (from librosa)\n",
            "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting lazy-loader>=0.1 (from librosa)\n",
            "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting msgpack>=1.0 (from librosa)\n",
            "  Using cached msgpack-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting packaging (from lazy-loader>=0.1->librosa)\n",
            "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting platformdirs>=2.5.0 (from pooch>=1.1->librosa)\n",
            "  Downloading platformdirs-4.3.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting requests>=2.19.0 (from pooch>=1.1->librosa)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.20.0->librosa)\n",
            "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting cffi>=1.0 (from soundfile>=0.12.1->librosa)\n",
            "  Downloading cffi-1.17.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting pycparser (from cffi>=1.0->soundfile>=0.12.1->librosa)\n",
            "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
            "  Downloading charset_normalizer-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading numba-0.60.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
            "Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
            "Downloading llvmlite-0.43.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached msgpack-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (377 kB)\n",
            "Using cached numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
            "Using cached scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "Using cached scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
            "Downloading soxr-0.5.0.post1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading cffi-1.17.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (445 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m445.2/445.2 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached packaging-24.2-py3-none-any.whl (65 kB)\n",
            "Downloading platformdirs-4.3.6-py3-none-any.whl (18 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m146.2/146.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, typing-extensions, threadpoolctl, pycparser, platformdirs, packaging, numpy, msgpack, llvmlite, joblib, idna, decorator, charset-normalizer, certifi, audioread, soxr, scipy, requests, numba, lazy-loader, cffi, soundfile, scikit-learn, pooch, librosa\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.18\n",
            "    Uninstalling urllib3-1.26.18:\n",
            "      Successfully uninstalled urllib3-1.26.18\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.5.0\n",
            "    Uninstalling threadpoolctl-3.5.0:\n",
            "      Successfully uninstalled threadpoolctl-3.5.0\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.21\n",
            "    Uninstalling pycparser-2.21:\n",
            "      Successfully uninstalled pycparser-2.21\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 3.10.0\n",
            "    Uninstalling platformdirs-3.10.0:\n",
            "      Successfully uninstalled platformdirs-3.10.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.1\n",
            "    Uninstalling packaging-23.1:\n",
            "      Successfully uninstalled packaging-23.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.1.0\n",
            "    Uninstalling msgpack-1.1.0:\n",
            "      Successfully uninstalled msgpack-1.1.0\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.39.1\n",
            "    Uninstalling llvmlite-0.39.1:\n",
            "      Successfully uninstalled llvmlite-0.39.1\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 5.1.1\n",
            "    Uninstalling decorator-5.1.1:\n",
            "      Successfully uninstalled decorator-5.1.1\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.0.4\n",
            "    Uninstalling charset-normalizer-2.0.4:\n",
            "      Successfully uninstalled charset-normalizer-2.0.4\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: audioread\n",
            "    Found existing installation: audioread 3.0.1\n",
            "    Uninstalling audioread-3.0.1:\n",
            "      Successfully uninstalled audioread-3.0.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.56.4\n",
            "    Uninstalling numba-0.56.4:\n",
            "      Successfully uninstalled numba-0.56.4\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.16.0\n",
            "    Uninstalling cffi-1.16.0:\n",
            "      Successfully uninstalled cffi-1.16.0\n",
            "  Attempting uninstall: soundfile\n",
            "    Found existing installation: soundfile 0.13.1\n",
            "    Uninstalling soundfile-0.13.1:\n",
            "      Successfully uninstalled soundfile-0.13.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: pooch\n",
            "    Found existing installation: pooch 1.8.2\n",
            "    Uninstalling pooch-1.8.2:\n",
            "      Successfully uninstalled pooch-1.8.2\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.9.2\n",
            "    Uninstalling librosa-0.9.2:\n",
            "      Successfully uninstalled librosa-0.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 4.24.0 requires numpy~=1.0, but you have numpy 2.0.2 which is incompatible.\n",
            "torchmetrics 1.5.0 requires numpy<2.0,>1.20.0, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed audioread-3.0.1 certifi-2025.1.31 cffi-1.17.1 charset-normalizer-3.4.1 decorator-5.1.1 idna-3.10 joblib-1.4.2 lazy-loader-0.4 librosa-0.10.2.post1 llvmlite-0.43.0 msgpack-1.1.0 numba-0.60.0 numpy-2.0.2 packaging-24.2 platformdirs-4.3.6 pooch-1.8.2 pycparser-2.22 requests-2.32.3 scikit-learn-1.6.1 scipy-1.13.1 soundfile-0.13.1 soxr-0.5.0.post1 threadpoolctl-3.5.0 typing-extensions-4.12.2 urllib3-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi"
                ]
              },
              "id": "15e6680c4997445c95184f9a922ede15"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy"
      ],
      "metadata": {
        "id": "GXh-ukLBkXGJ",
        "outputId": "69063d1b-912a-4ccb-d41f-3fa78eb82041",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/numpy-config\n",
            "    /usr/local/lib/python3.9/site-packages/numpy-2.0.2.dist-info/*\n",
            "    /usr/local/lib/python3.9/site-packages/numpy.libs/libgfortran-040039e1-0352e75f.so.5.0.0\n",
            "    /usr/local/lib/python3.9/site-packages/numpy.libs/libquadmath-96973f99-934c22de.so.0.0.0\n",
            "    /usr/local/lib/python3.9/site-packages/numpy.libs/libscipy_openblas64_-99b71e71.so\n",
            "    /usr/local/lib/python3.9/site-packages/numpy/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled numpy-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.4"
      ],
      "metadata": {
        "id": "3ilimxKzkZka",
        "outputId": "1c422eb8-defa-44d4-a1da-98be463366fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.4\n",
            "  Using cached numpy-1.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Using cached numpy-1.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "py3langid 0.3.0 requires numpy>=2.0.0; python_version >= \"3.9\", but you have numpy 1.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.4\n"
          ]
        }
      ]
    }
  ]
}